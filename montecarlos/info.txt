Monte Carlos learn based on experience by sampling episodes or trajectories of the agent's interactions with the environment. From there, it is able to find the optimal state or q-values.

To estimate the value of a state, Monte Carlo methods average the observed returns obtained from all episodes in which the state was encountered. Similarly, to estimate the value of an action, 
the observed returns for episodes in which the action was taken from a particular state are averaged. These estimates provide valuable insights into the quality of states or actions, 
which enable the agent to make informed decisions.

Advantages: 
model-free, meaning that they do not require a complete understanding of the underlying dynamics of the environment 
cost of estimating a state value is independent on the total number of states by focusing on a subset of states that are relevant to the task at hand

Disadvantages:
computationally intensive: often requires a large number of iterations and values are updated only after each episode, convergence to an optimal policy is slow
not suited for non-episodic tasks, where the agent interacts with the environment continuously without episodes